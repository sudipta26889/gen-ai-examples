{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5abc321",
   "metadata": {},
   "source": [
    "# Multi modality Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb167d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy torch transformers datasets opencv-python\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from utils.video_downloader import download_videos, get_available_formats\n",
    "from utils.dataloader import create_dataloader\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbefc6c",
   "metadata": {},
   "source": [
    "### First thing first, load a multimodal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57757168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 7010\n",
      "Dataset({\n",
      "    features: ['video_id', 'video', 'caption', 'source', 'category', 'url', 'start time', 'end time', 'id'],\n",
      "    num_rows: 7010\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def load_multimodal_dataset(dataset_name: str, split: str = \"train\"):\n",
    "    \"\"\"\n",
    "    Load a multimodal dataset from the Hugging Face Hub.\n",
    "    \"\"\"\n",
    "    dataset_dict = load_dataset(dataset_name, split)\n",
    "\n",
    "    # print(f\"Available splits: {list(dataset_dict.keys())}\")\n",
    "\n",
    "    if 'train' in dataset_dict:\n",
    "        dataset = dataset_dict['train']\n",
    "    else:\n",
    "        # If no 'train' split, use the first available split\n",
    "        first_split = list(dataset_dict.keys())[0]\n",
    "        dataset = dataset_dict[first_split]\n",
    "        print(f\"Using split '{first_split}' (no 'train' split found)\")\n",
    "    \n",
    "    print(f\"Original dataset size: {len(dataset)}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = load_multimodal_dataset(\"friedrichor/MSR-VTT\", split=\"train_7k\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b1fed",
   "metadata": {},
   "source": [
    "### Let's analyse the dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef33e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Features: ['video_id', 'video', 'caption', 'source', 'category', 'url', 'start time', 'end time', 'id']\n",
      "Dataset size: 7010\n",
      "Sample keys: dict_keys(['video_id', 'video', 'caption', 'source', 'category', 'url', 'start time', 'end time', 'id'])\n",
      "Sample values: {'video_id': 'video4907', 'video': 'video4907.mp4', 'caption': ['two guys dressed as spiderman are on a basketball court', 'a man in superman mask is talking to another masked man', 'there is a man is walking through the street', 'there is a man with mask is talking with the same', 'there is a mask man is talking with the same', 'spider man starts a fight with a guy in a costume', 'two spidermen meet in the court with ball and have an argument', 'a man in a spiderman costume is standing on a basketball court with another man in a deadpool costume', 'a man walks by a rough-textured wall with a door which turns wavy and results in two superheroes revealing their identity in a park', 'a man with a spiderman costume interacts with another faux super hero on a basketball court they take their masks off', 'two men were wears spider man dress and talking something', 'two men were wears spider man dress and talking something', 'two young men in super hero costumes talk about meeting before', 'spiderman and some other super hero get into a scuffle on street', 'two guys wearing super hero costumes stand on a basketball court and talk to one another', 'person walking then had day dream of being spider and with other super hero then they take masks off', 'the spider man fantasy spreading to fight against terrorism', 'men who are dressed like superheroes stand outside', 'two persons are dressed like a spiderman and standing in the basketball ground', 'couple of guys in spiderman costume playing basketball'], 'source': 'MSR-VTT', 'category': 3, 'url': 'https://www.youtube.com/watch?v=ZMN2Bs_Cr1A', 'start time': 513.77, 'end time': 528.2, 'id': 4907}\n",
      "\n",
      "Caption field type: <class 'list'>\n",
      "Sample caption: ['two guys dressed as spiderman are on a basketball court', 'a man in superman mask is talking to another masked man', 'there is a man is walking through the street', 'there is a man with mask is talking with the same', 'there is a mask man is talking with the same', 'spider man starts a fight with a guy in a costume', 'two spidermen meet in the court with ball and have an argument', 'a man in a spiderman costume is standing on a basketball court with another man in a deadpool costume', 'a man walks by a rough-textured wall with a door which turns wavy and results in two superheroes revealing their identity in a park', 'a man with a spiderman costume interacts with another faux super hero on a basketball court they take their masks off', 'two men were wears spider man dress and talking something', 'two men were wears spider man dress and talking something', 'two young men in super hero costumes talk about meeting before', 'spiderman and some other super hero get into a scuffle on street', 'two guys wearing super hero costumes stand on a basketball court and talk to one another', 'person walking then had day dream of being spider and with other super hero then they take masks off', 'the spider man fantasy spreading to fight against terrorism', 'men who are dressed like superheroes stand outside', 'two persons are dressed like a spiderman and standing in the basketball ground', 'couple of guys in spiderman costume playing basketball']\n",
      "\n",
      "=== Caption Analysis ===\n",
      "Average caption length: 9.64 words\n",
      "Min caption length: 3 words\n",
      "Max caption length: 41 words\n",
      "Median caption length: 9.00 words\n",
      "\n",
      "=== Video Analysis ===\n",
      "Total samples: 7010\n",
      "Unique videos: 7010\n",
      "Average captions per video: 1.00\n",
      "\n",
      "=== Category Analysis ===\n",
      "Unique categories: 20\n",
      "Categories: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "\n",
      "=== Available Fields ===\n",
      "video_id: <class 'str'> - video4907...\n",
      "video: <class 'str'> - video4907.mp4...\n",
      "caption: <class 'list'> - ['two guys dressed as spiderman are on a basketball court', 'a man in superman mask is talking to an...\n",
      "source: <class 'str'> - MSR-VTT...\n",
      "category: <class 'int'> - 3...\n",
      "url: <class 'str'> - https://www.youtube.com/watch?v=ZMN2Bs_Cr1A...\n",
      "start time: <class 'float'> - 513.77...\n",
      "end time: <class 'float'> - 528.2...\n",
      "id: <class 'int'> - 4907...\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset_structure(dataset):\n",
    "    print(f\"Dataset Features: {list(dataset.features.keys())}\")\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "    random_sample_data = dataset[random.randint(0, len(dataset))]\n",
    "\n",
    "    print(f\"Sample keys: {random_sample_data.keys()}\")\n",
    "    print(f\"Sample values: {random_sample_data}\")\n",
    "\n",
    "    # First, let's examine the structure of captions\n",
    "    random_sample_data_caption = random_sample_data['caption']\n",
    "    print(f\"\\nCaption field type: {type(random_sample_data_caption)}\")\n",
    "    print(f\"Sample caption: {random_sample_data_caption}\")\n",
    "\n",
    "    # Handle different caption formats\n",
    "    if isinstance(random_sample_data_caption, list):\n",
    "        # If captions are lists, flatten them or take first element\n",
    "        captions = []\n",
    "        for item in dataset:\n",
    "            caption_list = item['caption']\n",
    "            if isinstance(caption_list, list) and len(caption_list) > 0:\n",
    "                captions.append(caption_list[0])  # Take first caption\n",
    "            else:\n",
    "                captions.append(str(caption_list))\n",
    "    else:\n",
    "        # If captions are strings\n",
    "        captions = [item['caption'] for item in dataset]\n",
    "    \n",
    "    # Calculate caption lengths\n",
    "    caption_lengths = [len(str(caption).split()) for caption in captions]\n",
    "    \n",
    "    print(f\"\\n=== Caption Analysis ===\")\n",
    "    print(f\"Average caption length: {np.mean(caption_lengths):.2f} words\")\n",
    "    print(f\"Min caption length: {min(caption_lengths)} words\")\n",
    "    print(f\"Max caption length: {max(caption_lengths)} words\")\n",
    "    print(f\"Median caption length: {np.median(caption_lengths):.2f} words\")\n",
    "\n",
    "    # Analyze video IDs\n",
    "    video_ids = [item['video_id'] for item in dataset]\n",
    "    unique_videos = len(set(video_ids))\n",
    "    print(f\"\\n=== Video Analysis ===\")\n",
    "    print(f\"Total samples: {len(video_ids)}\")\n",
    "    print(f\"Unique videos: {unique_videos}\")\n",
    "    print(f\"Average captions per video: {len(video_ids) / unique_videos:.2f}\")\n",
    "    \n",
    "    # Check for categories if available\n",
    "    if 'category' in dataset.features:\n",
    "        categories = [item['category'] for item in dataset]\n",
    "        unique_categories = set(categories)\n",
    "        print(f\"\\n=== Category Analysis ===\")\n",
    "        print(f\"Unique categories: {len(unique_categories)}\")\n",
    "        print(f\"Categories: {sorted(unique_categories)}\")\n",
    "\n",
    "    # Check for other fields\n",
    "    print(f\"\\n=== Available Fields ===\")\n",
    "    for key, value in random_sample_data.items():\n",
    "        print(f\"{key}: {type(value)} - {str(value)[:100]}...\")\n",
    "    \n",
    "    return {\n",
    "        'caption_lengths': caption_lengths,\n",
    "        'unique_videos': unique_videos,\n",
    "        'sample_fields': list(random_sample_data.keys()),\n",
    "        'total_samples': len(dataset),\n",
    "        'processed_captions': captions\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze the dataset\n",
    "analysis = analyze_dataset_structure(dataset)\n",
    "\n",
    "# print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2826ba9",
   "metadata": {},
   "source": [
    "### Now, we have a basic understanding of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f99394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing 10 captions with bert-base-uncased\n",
      "\n",
      "Original captions (first 3):\n",
      "1: a car is shown\n",
      "2: in a kitchen a woman adds different ingredients into the pot and stirs it\n",
      "3: a guying showing a tool\n",
      "\n",
      "Tokenized output shape:\n",
      "Input IDs: torch.Size([10, 17])\n",
      "Attention mask: torch.Size([10, 17])\n",
      "\n",
      "Tokenized example (first caption):\n",
      "Tokens: ['[CLS]', 'a', 'car', 'is', 'shown', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']...\n",
      "\n",
      "Tokenization statistics:\n",
      "Average tokens per caption: 10.50\n",
      "Max tokens used: 17\n",
      "Min tokens used: 6\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_captions(captions: List[str], tokenizer_name: str = \"bert-base-uncased\", max_length: int = 77) -> Dict:\n",
    "    print(f\"Preprocessing {len(captions)} captions with {tokenizer_name}\")\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Show original captions\n",
    "    print(\"\\nOriginal captions (first 3):\")\n",
    "    for i, caption in enumerate(captions[:3]):\n",
    "        print(f\"{i+1}: {caption}\")\n",
    "    \n",
    "    # Tokenize captions\n",
    "    tokenized = tokenizer(\n",
    "        captions,\n",
    "        padding=True,           # Pad to same length\n",
    "        truncation=True,        # Truncate if too long\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"     # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTokenized output shape:\")\n",
    "    print(f\"Input IDs: {tokenized['input_ids'].shape}\")\n",
    "    print(f\"Attention mask: {tokenized['attention_mask'].shape}\")\n",
    "    \n",
    "    # Show tokenized example\n",
    "    print(f\"\\nTokenized example (first caption):\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized['input_ids'][0])\n",
    "    print(f\"Tokens: {tokens[:15]}...\")  # Show first 15 tokens\n",
    "    \n",
    "    # Analyze tokenization statistics\n",
    "    token_lengths = tokenized['attention_mask'].sum(dim=1)\n",
    "    print(f\"\\nTokenization statistics:\")\n",
    "    print(f\"Average tokens per caption: {token_lengths.float().mean():.2f}\")\n",
    "    print(f\"Max tokens used: {token_lengths.max().item()}\")\n",
    "    print(f\"Min tokens used: {token_lengths.min().item()}\")\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask'],\n",
    "        'tokenizer': tokenizer,\n",
    "        'token_lengths': token_lengths\n",
    "    }\n",
    "\n",
    "# Test text preprocessing with actual captions from dataset\n",
    "sample_captions = analysis['processed_captions'][:10]  # Use processed captions from analysis\n",
    "text_data = preprocess_text_captions(sample_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8dc36a",
   "metadata": {},
   "source": [
    "### Download Video to analyse it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "340d2af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting available formats for video2383...\n",
      "Available formats: ['269', '230']\n",
      "Trying format ID: 269\n",
      "Downloaded: video2383 with format 269\n",
      "videos/video2383.mp4\n"
     ]
    }
   ],
   "source": [
    "random_sample_data = dataset[random.randint(0, len(dataset))]\n",
    "downloaded_path = download_videos(random_sample_data)\n",
    "print(downloaded_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac0212",
   "metadata": {},
   "source": [
    "---\n",
    "## Till Now:\n",
    "* Loaded Multimodal Dataset \"friedrichor/MSR-VTT\" from HuggingFace\n",
    "* Analysed dataset structure Like:\n",
    "    * Dataset Features: columns/keys present in the dataset\n",
    "    * Dataset Size \n",
    "    * Checked sample datas/values\n",
    "    * Also analysed caption key, and got that it's a list so further we analysed the captions like avg, min, max, median length of each caption.\n",
    "    * Checked total videos, toal unique videos, average captions per video \n",
    "    * Check categories of the videos etc.\n",
    "* Processed texts - in this case captions and tokenised them in same size.\n",
    "* Also Download Video mentioned in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba5157",
   "metadata": {},
   "source": [
    "### Time To Create Custom **MultimodalDataset** Class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48e412f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, video_dir=\"videos\", max_frames=8):\n",
    "        self.dataset = dataset\n",
    "        self.video_dir = video_dir\n",
    "        self.max_frames = max_frames\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # Find valid samples\n",
    "        self.valid_indices = []\n",
    "        for i in range(len(dataset)):\n",
    "            video_id = dataset[i]['video_id']\n",
    "            if os.path.exists(os.path.join(video_dir, f\"{video_id}.mp4\")):\n",
    "                self.valid_indices.append(i)\n",
    "        \n",
    "        print(f\"Found {len(self.valid_indices)} valid samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[self.valid_indices[idx]]\n",
    "        \n",
    "        # Process text\n",
    "        caption = sample.get('caption', '')\n",
    "        if isinstance(caption, list):\n",
    "            caption = caption[0]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            caption, padding='max_length', truncation=True, \n",
    "            max_length=77, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Load video frames using cv2\n",
    "        video_path = os.path.join(self.video_dir, f\"{sample['video_id']}.mp4\")\n",
    "        video_frames = self._load_video_cv2(video_path)\n",
    "        \n",
    "        return {\n",
    "            'text_ids': tokenized['input_ids'].squeeze(0),\n",
    "            'text_mask': tokenized['attention_mask'].squeeze(0),\n",
    "            'caption': caption,\n",
    "            'video_frames': video_frames,\n",
    "            'video_id': sample['video_id'],\n",
    "            'video': sample['video'],\n",
    "            'source': sample['source'],\n",
    "            'category': sample['category'],\n",
    "            'url': sample['url'],\n",
    "            'start_time': sample['start time'],\n",
    "            'end_time': sample['end time'],\n",
    "            'id': sample['id']\n",
    "        }\n",
    "    \n",
    "    def _load_video_cv2(self, video_path):\n",
    "        \"\"\"Load video frames using OpenCV with comprehensive preprocessing.\n",
    "        \n",
    "        This function loads a video file, extracts frames uniformly across the video duration,\n",
    "        preprocesses them for machine learning (resizing, color conversion, normalization),\n",
    "        and returns them as a PyTorch tensor suitable for multimodal model training.\n",
    "        \n",
    "        Args:\n",
    "            video_path (str): Full path to the video file (.mp4, .avi, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (max_frames, 3, 224, 224) containing\n",
    "                        RGB video frames normalized to [0, 1] range.\n",
    "                        \n",
    "        Raises:\n",
    "            Exception: If video cannot be loaded, falls back to placeholder frames.\n",
    "            \n",
    "        Processing Pipeline:\n",
    "            1. Open video file using OpenCV VideoCapture\n",
    "            2. Extract video metadata (total frames, fps)\n",
    "            3. Calculate uniform frame sampling indices\n",
    "            4. Extract and preprocess each frame:\n",
    "            - Convert from BGR to RGB color space\n",
    "            - Resize to 224x224 pixels (standard vision model input size)\n",
    "            - Normalize pixel values from [0, 255] to [0, 1] range\n",
    "            - Convert to PyTorch tensor with channels-first format (C, H, W)\n",
    "            5. Handle frame padding to ensure consistent output size\n",
    "            6. Stack all frames into a single tensor\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Initialize OpenCV VideoCapture object\n",
    "            # VideoCapture is the primary interface for reading video files in OpenCV\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            \n",
    "            # Step 2: Verify video file can be opened\n",
    "            # isOpened() returns True if the video source has been initialized successfully\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Could not open video: {video_path}\")\n",
    "                return None\n",
    "            \n",
    "            # Step 3: Extract video metadata for frame sampling strategy\n",
    "            # CAP_PROP_FRAME_COUNT: Total number of frames in the video\n",
    "            # CAP_PROP_FPS: Frames per second of the video\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            \n",
    "            # Step 4: Validate video properties\n",
    "            # Videos with 0 frames or invalid fps cannot be processed\n",
    "            if total_frames <= 0 or fps <= 0:\n",
    "                print(f\"Invalid video properties: frames={total_frames}, fps={fps}\")\n",
    "                cap.release()  # Always release resources\n",
    "                return None\n",
    "            \n",
    "            # Step 5: Calculate frame sampling indices for uniform temporal coverage\n",
    "            # We want to sample max_frames uniformly across the entire video duration\n",
    "            # This ensures we capture the video's temporal progression regardless of length\n",
    "            if total_frames <= self.max_frames:\n",
    "                # If video has fewer frames than needed, use all available frames\n",
    "                frame_indices = list(range(total_frames))\n",
    "            else:\n",
    "                # Calculate uniformly spaced indices across the video\n",
    "                # Formula: index = (sample_position * total_frames) / max_frames\n",
    "                # This gives us evenly distributed frames across the entire video\n",
    "                frame_indices = [\n",
    "                    int(i * total_frames / self.max_frames) \n",
    "                    for i in range(self.max_frames)\n",
    "                ]\n",
    "            \n",
    "            frames = []\n",
    "            \n",
    "            # Step 6: Extract and preprocess each sampled frame\n",
    "            for frame_idx in frame_indices:\n",
    "                # Step 6a: Seek to specific frame position\n",
    "                # CAP_PROP_POS_FRAMES sets the 0-based index of the frame to be decoded/captured next\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                \n",
    "                # Step 6b: Read the frame at current position\n",
    "                # ret: boolean indicating if frame was read successfully\n",
    "                # frame: numpy array containing the frame data (H, W, C) in BGR format\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                # Step 6c: Verify frame was read successfully\n",
    "                if ret and frame is not None:\n",
    "                    # Step 6d: Color space conversion from BGR to RGB\n",
    "                    # OpenCV uses BGR (Blue-Green-Red) by default, but most ML models expect RGB\n",
    "                    # This is crucial for correct color representation in the model\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Step 6e: Resize frame to standard input size\n",
    "                    # 224x224 is the standard input size for many vision models (ResNet, ViT, etc.)\n",
    "                    # Bicubic interpolation preserves image quality during resizing\n",
    "                    frame = cv2.resize(frame, (224, 224))\n",
    "                    \n",
    "                    # Step 6f: Convert to PyTorch tensor and normalize\n",
    "                    # Convert numpy array to PyTorch tensor with float32 precision\n",
    "                    # Permute dimensions from (H, W, C) to (C, H, W) - channels first format\n",
    "                    # Normalize pixel values from [0, 255] to [0, 1] range for numerical stability\n",
    "                    frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "                    \n",
    "                    frames.append(frame_tensor)\n",
    "                else:\n",
    "                    # Step 6g: Handle frame reading failure\n",
    "                    print(f\"Could not read frame {frame_idx}\")\n",
    "                    break  # Stop processing if we encounter read errors\n",
    "            \n",
    "            # Step 7: Release video capture resources\n",
    "            # Always release to prevent memory leaks and file locks\n",
    "            cap.release()\n",
    "            \n",
    "            # Step 8: Handle frame padding to ensure consistent output size\n",
    "            # If we have fewer frames than max_frames, pad with the last valid frame\n",
    "            # This ensures all samples have the same tensor dimensions for batching\n",
    "            while len(frames) < self.max_frames:\n",
    "                if frames:\n",
    "                    # Clone the last frame to avoid tensor sharing issues\n",
    "                    frames.append(frames[-1].clone())\n",
    "                else:\n",
    "                    # If no frames were successfully read, return None\n",
    "                    return None\n",
    "            \n",
    "            # Step 9: Ensure exact frame count by truncating if necessary\n",
    "            # This handles edge cases where we might have extracted more frames than needed\n",
    "            frames = frames[:self.max_frames]\n",
    "            \n",
    "            # Step 10: Stack individual frame tensors into a single tensor\n",
    "            # Result shape: (max_frames, 3, 224, 224)\n",
    "            # This creates a 4D tensor suitable for video processing models\n",
    "            return torch.stack(frames)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Step 11: Comprehensive error handling\n",
    "            # Catch any unexpected errors (codec issues, corrupted files, etc.)\n",
    "            print(f\"Error loading video {video_path}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde6b67",
   "metadata": {},
   "source": [
    "### Let's use MultimodalDataset to analyse videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4b900b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 valid samples\n",
      "Available keys:\n",
      "  - text_ids\n",
      "  - text_masks\n",
      "  - captions\n",
      "  - video_frames\n",
      "  - video_ids\n",
      "  - videos\n",
      "  - sources\n",
      "  - categories\n",
      "  - urls\n",
      "  - start_times\n",
      "  - end_times\n",
      "  - ids\n",
      "\n",
      "Batch size: 2\n",
      "Video shape: torch.Size([2, 8, 3, 224, 224])\n",
      "Text shape: torch.Size([2, 77])\n",
      "Video frames range: [0.000, 1.000]\n",
      "\n",
      "=== Video 1 ===\n",
      "URL: https://www.youtube.com/watch?v=trHNRK7NfUc\n",
      "Caption: a clip of a football team celebrating\n",
      "Video ID: video2383\n",
      "Source: MSR-VTT\n",
      "Category: 3\n",
      "Start time: 17.75\n",
      "End time: 28.81\n",
      "Duration: 11.1s\n",
      "\n",
      "=== Video 2 ===\n",
      "URL: https://www.youtube.com/watch?v=c0uW5eQqQjM\n",
      "Caption: a boy is playing a video game\n",
      "Video ID: video4471\n",
      "Source: MSR-VTT\n",
      "Category: 2\n",
      "Start time: 524.24\n",
      "End time: 535.65\n",
      "Duration: 11.4s\n"
     ]
    }
   ],
   "source": [
    "multimodal_dataset = MultimodalDataset(dataset, video_dir=\"videos\")\n",
    "dataloader = create_dataloader(multimodal_dataset, batch_size=2)\n",
    "\n",
    "# Test\n",
    "batch = next(iter(dataloader))\n",
    "print(\"Available keys:\")\n",
    "for key in batch.keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "print(f\"\\nBatch size: {len(batch['captions'])}\")\n",
    "print(f\"Video shape: {batch['video_frames'].shape}\")\n",
    "print(f\"Text shape: {batch['text_ids'].shape}\")\n",
    "print(f\"Video frames range: [{batch['video_frames'].min():.3f}, {batch['video_frames'].max():.3f}]\")\n",
    "\n",
    "# Display data for ALL videos in the batch\n",
    "for i in range(len(batch['captions'])):\n",
    "    print(f\"\\n=== Video {i+1} ===\")\n",
    "    print(f\"URL: {batch['urls'][i]}\")\n",
    "    print(f\"Caption: {batch['captions'][i]}\")\n",
    "    print(f\"Video ID: {batch['video_ids'][i]}\")\n",
    "    print(f\"Source: {batch['sources'][i]}\")\n",
    "    print(f\"Category: {batch['categories'][i]}\")\n",
    "    print(f\"Start time: {batch['start_times'][i]}\")\n",
    "    print(f\"End time: {batch['end_times'][i]}\")\n",
    "    print(f\"Duration: {batch['end_times'][i] - batch['start_times'][i]:.1f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
